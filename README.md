# ETL-Data-Pipeline
## In this Project, I built an ETL Data Pipeline. 

 A data pipeline is a systematics and automated process for the efficient and reliable movement, transformation and management of data from one point to another within a computing environment. It is a method in which raw data is ingested from various data sources, transformed and then ported to a data store such as a data lake, data warehouse or a database. 

ETL is an acronym for Extract, Transform and Load. The ETL process is how raw data is converted into analysis-ready data. It is an automated process in which you gather raw data from identified sources, extract the information that aligns with your reportibg and analysis needs, clean, standardize and transform that data into a format that is usable in the context of your organization and load it into a data repository. 

An ETL Data Pipeline is a type of data pipeline that is used to extract, transform and load data from various sources into a target system typically for the purpose of analysis, reporting or other related data tasks. It helps in consolidating and preparing data from multiple sources into a format suitable for analysis, reporting and business intelligence. 

